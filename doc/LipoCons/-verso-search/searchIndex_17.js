window.docContents[17].resolve({"/Consistency-and-sampling/#Consistency-of-global-optimization-algorithms--Consistency-and-sampling--Consistency-of-algorithms":{"contents":"In the paper, the consistency of an algorithm is defined using convergence in probability. A stochastic iterative global optimization algorithm is sait to be consistent over f if the maximum of the evaluations of the samples produced by the algorithm converges in probability to the maximum of f over the search space, i.e.\n\n\\max_{0 \\le i \\le n} f(X_i) \\xrightarrow{p} \\max_{x \\in \\mathcal{X}} f(x),\n\nwhere X_i is the i-th sample produced by the algorithm. Note that we consider here the case where an algorithm searches to maximize a function, but the definition can be adapted to the case of minimization.\n\nThe latter definition can be unfolded as follows:\n\n\\forall \\varepsilon > 0, \\; \\mathbb{P}(|\\max_{0 \\le i \\le n} f(X_i) - \\max_{x \\in \\mathcal{X}} f(x)| > \\varepsilon) \\xrightarrow[n \\to \\infty]{} 0,\n\nwhere \\mathbb{P} denotes the probability measure on sequences of samples of size n + 1 produced by the algorithm. In our formalization, we showed that \\mathbb{P} is equal to Algorithm..\n\nTo formally define the consistency using Algorithm., we define the set of sequences of samples of size n + 1 such that the maximum of the evaluations is at least \\varepsilon away from the maximum of f over the compact search space (noted ).\n\n\n\nWe can now define the measure of this set of sequences.\n\n\n\nFinally, an algorithm A is consistent over a Lipschitz function f if  tends to 0 when n tends to infinity.\n\n\n\n","context":"Consistency of global optimization algorithms\u0009Consistency and sampling","header":"2.1. Consistency of algorithms","id":"/Consistency-and-sampling/#Consistency-of-global-optimization-algorithms--Consistency-and-sampling--Consistency-of-algorithms"}});